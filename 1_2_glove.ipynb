{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d589f30",
   "metadata": {
    "id": "3d589f30"
   },
   "source": [
    "# 1.2 Glove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_c3qjPoaBHnm",
   "metadata": {
    "id": "_c3qjPoaBHnm"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/G1-ABID-23-24/offensive-language-detection-2024/blob/main/1.1_GloVe.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df6212b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df6212b2",
    "outputId": "e5973233-cb0f-4ff5-9246-1a3abce00e56"
   },
   "outputs": [],
   "source": [
    "#Import libraries and upload the dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#If you don't have en_core_web_lg downloaded (stopword list)\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "en_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cJoMI9cC5c_e",
   "metadata": {
    "id": "cJoMI9cC5c_e"
   },
   "outputs": [],
   "source": [
    "#Function to correct spelling errors\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(filter(None, corrected_text))\n",
    "        \n",
    "#Function to remove URLs\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "#Function to remove emojis from the text\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "#Function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "#Function to remove stopwords from the text\n",
    "def remove_stop_words(text):\n",
    "    cleanText = ''\n",
    "    phrase = nlp(text)\n",
    "    for token in phrase:\n",
    "        if not token.is_stop:\n",
    "        #and not token.is_punct and not token.like_url:\n",
    "            cleanText += ' ' + token.text\n",
    "    return cleanText\n",
    "\n",
    "#df['text']=df['text'].apply(lambda x : correct_spellings(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
    "#df['text']=df['text'].apply(lambda x : remove_emoji(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
    "df['text']=df['text'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5de25",
   "metadata": {
    "id": "d2f5de25"
   },
   "source": [
    "### Vectorización con GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f9e1ec2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f9e1ec2",
    "outputId": "7076f95c-b626-4f20-8228-b12910d29515",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8148/8148 [00:00<00:00, 13534.79it/s]\n"
     ]
    }
   ],
   "source": [
    "#Function to create a corpus for GloVe embedding\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in en_stopwords))]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus=create_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48461245",
   "metadata": {
    "id": "48461245"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"unzip\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munzip glove*.zip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m embedding_dict\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglove.6B.100d.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m         values\u001b[38;5;241m=\u001b[39mline\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip\n",
    "\n",
    "embedding_dict={}\n",
    "with open('glove.6B.100d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HB0gtDoJCVQ1",
   "metadata": {
    "id": "HB0gtDoJCVQ1"
   },
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TCT6KROKCYXD",
   "metadata": {
    "id": "TCT6KROKCYXD"
   },
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7zp-EyFkCcv1",
   "metadata": {
    "id": "7zp-EyFkCcv1"
   },
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,100))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iVNxg_YWCgOx",
   "metadata": {
    "id": "iVNxg_YWCgOx"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xn5gkJAAC2N_",
   "metadata": {
    "id": "Xn5gkJAAC2N_"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gKCjRMIkC5ZT",
   "metadata": {
    "id": "gKCjRMIkC5ZT"
   },
   "outputs": [],
   "source": [
    "train=tweet_pad[:tweet.shape[0]]\n",
    "test=tweet_pad[tweet.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mWahSkUj1lnX",
   "metadata": {
    "id": "mWahSkUj1lnX"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\n",
    "print('Shape of train',X_train.shape)\n",
    "print(\"Shape of Validation \",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GH4YtXtN6yUB",
   "metadata": {
    "id": "GH4YtXtN6yUB"
   },
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
