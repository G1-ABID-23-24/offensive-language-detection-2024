{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d589f30",
   "metadata": {
    "id": "3d589f30"
   },
   "source": [
    "# 1.2 Glove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_c3qjPoaBHnm",
   "metadata": {
    "id": "_c3qjPoaBHnm"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/G1-ABID-23-24/offensive-language-detection-2024/blob/main/1.1_GloVe.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287022d",
   "metadata": {},
   "source": [
    "Inicialmente se va a realizar un preprocesado de los datos, eliminando las palabras sin significado útil, los url y los signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6212b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "df6212b2",
    "outputId": "e5973233-cb0f-4ff5-9246-1a3abce00e56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import libraries and upload the dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau,CSVLogger\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,classification_report\n",
    "import shutil\n",
    "\n",
    "\n",
    "#If you don't have en_core_web_lg downloaded (stopword list)\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "en_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cJoMI9cC5c_e",
   "metadata": {
    "id": "cJoMI9cC5c_e"
   },
   "outputs": [],
   "source": [
    "#Function to correct spelling errors\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(filter(None, corrected_text))\n",
    "        \n",
    "#Function to remove URLs\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "#Function to remove emojis from the text\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "#Function to remove punctuation\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "#Function to remove stopwords from the text\n",
    "def remove_stop_words(text):\n",
    "    cleanText = ''\n",
    "    phrase = nlp(text)\n",
    "    for token in phrase:\n",
    "        if not token.is_stop:\n",
    "        #and not token.is_punct and not token.like_url:\n",
    "            cleanText += ' ' + token.text\n",
    "    return cleanText\n",
    "\n",
    "#df['text']=df['text'].apply(lambda x : correct_spellings(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
    "#df['text']=df['text'].apply(lambda x : remove_emoji(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
    "df['text']=df['text'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0b9e0",
   "metadata": {},
   "source": [
    "Despues de esto, hacemos las transformaciones necesarias para usar la libreria de Glove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dc4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df['text']\n",
    "y=df['label']\n",
    "\n",
    "texts = x\n",
    "target = y\n",
    "\n",
    "#tokenising the data\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(texts)\n",
    "#defining vocabulary length\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "def embed(text_data): \n",
    "    return word_tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "longest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f5de25",
   "metadata": {
    "id": "d2f5de25"
   },
   "source": [
    "### Vectorización con GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e1ec2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f9e1ec2",
    "outputId": "7076f95c-b626-4f20-8228-b12910d29515",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Function to create a corpus for GloVe embedding\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in en_stopwords))]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus=create_corpus(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608713ab",
   "metadata": {},
   "source": [
    "A continuación, comenzamos descargando el modelo más básico de Glove disponible, ya que no hará falta uno más complicado, despues comenzamos a adecuar nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48461245",
   "metadata": {
    "id": "48461245"
   },
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip\n",
    "\n",
    "embedding_dict={}\n",
    "with open('glove.6B/glove.6B.100d.txt','r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01535684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding_vectors of words which comes in Glove files other will be equated to 0\n",
    "#defining embedding matrix shape\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "#creating embedding matrix\n",
    "for word, index in word_tokenizer.word_index.items(): \n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        \n",
    "#splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_padded_sentences, \n",
    "    target, \n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "X_train, x_val, y_train, y_val = train_test_split(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    test_size=0.1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49d846",
   "metadata": {},
   "source": [
    "Comenzamos probando nuestros datos con el algoritmo BiLSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76874ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining glove bilstm model\n",
    "def bilstm():\n",
    "    model = Sequential()\n",
    "    #adding embediing layer\n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0], \n",
    "        output_dim=embedding_matrix.shape[1], \n",
    "        weights = [embedding_matrix], \n",
    "        input_length=length_long_sentence))\n",
    "    #adding Bi_lstm later\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        length_long_sentence, \n",
    "        return_sequences = True, \n",
    "        recurrent_dropout=0.2)))\n",
    "    model.add(GlobalMaxPool1D()) #globalmaxpooling_layer\n",
    "    model.add(BatchNormalization()) #bath_normalisation\n",
    "    model.add(Dropout(0.5)) #dropout_1\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\")) #denselayer_1\n",
    "    model.add(Dropout(0.5)) #dropout_2\n",
    "    model.add(Dense(length_long_sentence, activation = \"relu\")) #denselayer_2\n",
    "    model.add(Dropout(0.5)) #dropout_3\n",
    "    model.add(Dense(3, activation = 'softmax')) #classification_layer\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "bilstm_model = bilstm()\n",
    "\n",
    "#defining_class_weight for each class\n",
    "weight_class1 = (1 / hate)*(total)/3.0 \n",
    "weight_class2 = (1 / ofensive)*(total)/3.0\n",
    "weight_class3 = (1 / neither)*(total)/3.0\n",
    "class_weight = {0: weight_class1, 1: weight_class2, 2: weight_class3}\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', \n",
    "    factor = 0.2, \n",
    "    verbose = 1, \n",
    "    patience = 5,                        \n",
    "    min_lr = 0.001\n",
    ")\n",
    "\n",
    "epoch_count=20\n",
    "batch_size= 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running_model\n",
    "history = bilstm_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = epoch_count,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = (x_val, y_val),\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "#plotting graphs\n",
    "def plot_learning_curves(history, arr):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "    for idx in range(2):\n",
    "        ax[idx].plot(history.history[arr[idx][0]])\n",
    "        ax[idx].plot(history.history[arr[idx][1]])\n",
    "        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=17)\n",
    "        ax[idx].set_xlabel('Loss ',fontsize=14)\n",
    "        ax[idx].set_ylabel('Accuracy',fontsize=14)\n",
    "        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)\n",
    "\n",
    "plot_learning_curves(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n",
    "\n",
    "#prediciting\n",
    "preds= np.argmax(model.predict(X_test), axis=-1)\n",
    "#printing classification_report & confusion_matrix\n",
    "print(classification_report(y_test,preds ))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df89dd",
   "metadata": {},
   "source": [
    "Ahora, con redes convolucionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d0c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filters= 32\n",
    "kernel_size=2\n",
    "hidden_dims= 128\n",
    "    \n",
    "def CNN():\n",
    "    model = Sequential()\n",
    "    #adding embedding layer\n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0], \n",
    "        output_dim=embedding_matrix.shape[1], \n",
    "        weights = [embedding_matrix], \n",
    "        input_length=length_long_sentence))\n",
    "    # 2 CNN layer\n",
    "    model.add(Conv1D(32,2,padding='valid', activation='relu')) #cnn_layer_1\n",
    "    model.add(Conv1D(64,2,padding='valid',activation='relu')) #cnn_layer_2\n",
    "    model.add(GlobalMaxPooling1D()) #globalmaxpooling_layer\n",
    "    model.add(Dense(256, activation='relu')) #dense_layer\n",
    "    model.add(Dropout(0.1)) #dropout_layer\n",
    "    model.add(Dense(3, activation = 'softmax')) #classification layer\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "#builiding CNN model\n",
    "model2=CNN()\n",
    " \n",
    "#running mode\n",
    "history2 = model2.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = epoch_count,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = (x_val, y_val),\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr, checkpoint],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "#plotting graphs\n",
    "plot_learning_curves(history2, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n",
    "#predicting\n",
    "pred2= np.argmax(model2.predict(X_test), axis=-1)\n",
    "#printing reports\n",
    "print(classification_report(y_test,pred2 ))\n",
    "print(confusion_matrix(y_test, pred2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5599b9e",
   "metadata": {},
   "source": [
    "Terminamos con MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP():\n",
    "    model = Sequential()\n",
    "    #embedding layer\n",
    "    model.add(Embedding(\n",
    "        input_dim=embedding_matrix.shape[0], \n",
    "        output_dim=embedding_matrix.shape[1], \n",
    "        weights = [embedding_matrix], \n",
    "        input_length=length_long_sentence))\n",
    "    model.add(Flatten()) #flatten_layer\n",
    "    model.add(Dense(512, activation='relu')) #dense_layer\n",
    "    model.add(Dropout(0.2)) #dropout_layer\n",
    "    model.add(Dense(3, activation = 'softmax'))#classification_layer\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "#building model\n",
    "model3=MLP()\n",
    "#running_model\n",
    "history3 = model3.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs = epoch_count,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = (x_val, y_val),\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr, checkpoint],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "#plotting_graphs\n",
    "plot_learning_curves(history3, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])\n",
    "#predicting\n",
    "pred3= np.argmax(model3.predict(X_test), axis=-1)\n",
    "\n",
    "print(classification_report(y_test,pred3))\n",
    "print(confusion_matrix(y_test, pred3))e"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
